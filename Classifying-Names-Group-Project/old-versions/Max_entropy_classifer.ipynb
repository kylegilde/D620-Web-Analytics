{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import repeat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import names\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['female.txt', 'male.txt']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm male and female txt files exist\n",
    "names.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load male and female  name files from nltk.names; store in people list\n",
    "males = [n for n in names.words('male.txt')] \n",
    "females = [n for n in names.words('female.txt')] \n",
    "people = males + females\n",
    "\n",
    "# make gender list\n",
    "gender = list(repeat('male',len(males))) + \\\n",
    "list(repeat('female',len(females)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce features\n",
    "def gender_features(word, *args):\n",
    "    \"\"\"\n",
    "    function returns dictionary of features\n",
    "        word: name to extract features from\n",
    "        args:  one or more strings to specify desired features, including:\n",
    "                'length','first','first2','first3', 'last', 'last2', 'last3',\n",
    "                'every_other2_beg','every_other3_beg', 'every_other2_end', 'every_other3_end',\n",
    "                'vowel_ct', 'round_cons_ct', 'sharp_cons_ct','round_vowel_ct',\n",
    "                trad_female_end'\n",
    "               \n",
    "    \"\"\"\n",
    "    \n",
    "    gf = {}\n",
    "    \n",
    "    # word length\n",
    "    gf['length'] = len(word)\n",
    "   \n",
    "    # first letters\n",
    "    gf['first'] = word[0].lower()\n",
    "    gf['first2'] = word[0:2].lower()\n",
    "    gf['first3'] = word[0:3].lower() if gf['length'] >2  else word[0:2].lower()\n",
    "    \n",
    "    # last letters\n",
    "    gf['last'] = word[-1].lower()\n",
    "    gf['last2'] = word[-2:].lower()\n",
    "    gf['last3'] = word[-3:].lower() if gf['length'] >2  else word[-2:].lower()\n",
    "    \n",
    "    # every other beg\n",
    "    gf['every_other2_beg'] = word[0]+word[2] if gf['length'] > 2 else word[0]\n",
    "    gf['every_other3_beg'] = gf['every_other2_beg']+word[4]  if gf['length'] > 4 else \\\n",
    "    gf['every_other2_beg']\n",
    "    \n",
    "    # every other end\n",
    "    gf['every_other2_end'] = word[-3]+word[-1] if gf['length'] > 2 else word[-1]\n",
    "    gf['every_other3_end'] = word[-5]+gf['every_other2_end']  if gf['length'] > 4 else \\\n",
    "    gf['every_other2_end']\n",
    "    \n",
    "    # count: vowels, rounded consonants, sharp consonants\n",
    "    for letter in word:\n",
    "        # count vowels\n",
    "        if letter in 'aeiou':\n",
    "            gf['vowel_ct'] = gf.get('vowel_ct',0) + 1\n",
    "        # count rounded consonants\n",
    "        if letter in 'bmln':\n",
    "            gf['round_cons_ct'] = gf.get('round_cons_ct',0) + 1\n",
    "        # count sharp consonants\n",
    "        if letter in 'k,p,t':\n",
    "            gf['sharp_cons_ct'] = gf.get('sharp_cons_ct',0) + 1\n",
    "        # count rounded vowels\n",
    "        if letter in 'uo':\n",
    "            gf['round_vowel_ct'] = gf.get('round_vowel_ct',0) + 1\n",
    "            \n",
    "    # traditional feminine ending, 'y' or 'n'\n",
    "    gf['trad_female_end'] = 'y' if gf['last2'] in ['ie','ah'] or \\\n",
    "    gf['last'] in ['a','y'] else 'n'\n",
    "    \n",
    "    # generate dictionary subset\n",
    "    return(dict((k, gf[k]) for k in args if k in gf))\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'every_other2_beg': 'Sn',\n",
       " 'every_other2_end': 'ny',\n",
       " 'every_other3_beg': 'Sny',\n",
       " 'every_other3_end': 'Sny',\n",
       " 'first': 's',\n",
       " 'first2': 'sa',\n",
       " 'first3': 'san',\n",
       " 'last': 'y',\n",
       " 'last2': 'dy',\n",
       " 'last3': 'ndy',\n",
       " 'length': 5,\n",
       " 'round_cons_ct': 1,\n",
       " 'trad_female_end': 'y',\n",
       " 'vowel_ct': 1}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify which features to use\n",
    "myargs = ['length','first','first2','first3', 'last', 'last2', 'last3', \\\n",
    "          'every_other2_beg','every_other3_beg', 'every_other2_end', 'every_other3_end', \\\n",
    "          'vowel_ct', 'round_cons_ct', 'sharp_cons_ct','round_vowel_ct', \\\n",
    "          'trad_female_end']\n",
    "\n",
    "# specify name, and argument list \n",
    "gender_features('Sandy', *myargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into test and train, with test file containing 1000 samples\n",
    "people_train, people_test, gender_train, gender_test =  \\\n",
    "train_test_split(people, gender, test_size=1000, random_state=4)\n",
    "\n",
    "# split test into two separate components of 500 each: test and devtest\n",
    "people_test, people_devtest, gender_test, gender_devtest = \\\n",
    "train_test_split(people_test, gender_test, test_size=500, random_state=4)\n",
    "\n",
    "# list of tuples, gender features, gender\n",
    "train_set = list(zip(map(lambda d: gender_features(d, *myargs), people_train),gender_train))\n",
    "devtest_set = list(zip(map(lambda d: gender_features(d, *myargs), people_devtest),gender_devtest))\n",
    "test_set = list(zip(map(lambda d: gender_features(d, *myargs), people_test),gender_test))\n",
    "\n",
    "\n",
    "# list of tuples, names, gender\n",
    "train_names = list(zip(people_train,gender_train))\n",
    "devtest_names = list(zip(people_devtest,gender_devtest))\n",
    "test_names = list(zip(people_test, gender_test))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's  calculate the entropy of the labels in our dataset. The higher the entropy the better our classification algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951030970454714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def entropy(labels):    \n",
    "    freqdist = nltk.FreqDist(labels)    \n",
    "    probs = [freqdist.freq(l) for l in nltk.FreqDist(labels)]    \n",
    "    return -sum([p * math.log(p,2) for p in probs])\n",
    "\n",
    "print (entropy(gender))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 95% entropy which very good\n",
    "\n",
    "Let's now build a maximum entropy classifier based on the features. By default it will run for 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.368\n",
      "             2          -0.42676        0.822\n",
      "             3          -0.34445        0.877\n",
      "             4          -0.29924        0.891\n",
      "             5          -0.27015        0.903\n",
      "             6          -0.24939        0.908\n",
      "             7          -0.23355        0.914\n",
      "             8          -0.22087        0.919\n",
      "             9          -0.21039        0.923\n",
      "            10          -0.20150        0.926\n",
      "            11          -0.19381        0.929\n",
      "            12          -0.18707        0.931\n",
      "            13          -0.18109        0.933\n",
      "            14          -0.17572        0.935\n",
      "            15          -0.17087        0.936\n",
      "            16          -0.16646        0.939\n",
      "            17          -0.16243        0.940\n",
      "            18          -0.15871        0.940\n",
      "            19          -0.15528        0.942\n",
      "            20          -0.15210        0.943\n",
      "            21          -0.14914        0.943\n",
      "            22          -0.14637        0.944\n",
      "            23          -0.14377        0.944\n",
      "            24          -0.14134        0.945\n",
      "            25          -0.13904        0.946\n",
      "            26          -0.13688        0.947\n",
      "            27          -0.13483        0.948\n",
      "            28          -0.13289        0.948\n",
      "            29          -0.13105        0.949\n",
      "            30          -0.12930        0.949\n",
      "            31          -0.12763        0.949\n",
      "            32          -0.12604        0.949\n",
      "            33          -0.12452        0.949\n",
      "            34          -0.12307        0.950\n",
      "            35          -0.12168        0.950\n",
      "            36          -0.12035        0.950\n",
      "            37          -0.11907        0.951\n",
      "            38          -0.11784        0.951\n",
      "            39          -0.11666        0.951\n",
      "            40          -0.11552        0.952\n",
      "            41          -0.11442        0.952\n",
      "            42          -0.11337        0.952\n",
      "            43          -0.11235        0.952\n",
      "            44          -0.11136        0.952\n",
      "            45          -0.11041        0.952\n",
      "            46          -0.10949        0.953\n",
      "            47          -0.10860        0.953\n",
      "            48          -0.10774        0.953\n",
      "            49          -0.10691        0.953\n",
      "            50          -0.10610        0.953\n",
      "            51          -0.10531        0.953\n",
      "            52          -0.10455        0.953\n",
      "            53          -0.10381        0.953\n",
      "            54          -0.10309        0.953\n",
      "            55          -0.10239        0.954\n",
      "            56          -0.10171        0.954\n",
      "            57          -0.10105        0.954\n",
      "            58          -0.10041        0.954\n",
      "            59          -0.09978        0.954\n",
      "            60          -0.09917        0.954\n",
      "            61          -0.09857        0.954\n",
      "            62          -0.09799        0.954\n",
      "            63          -0.09743        0.954\n",
      "            64          -0.09687        0.954\n",
      "            65          -0.09633        0.954\n",
      "            66          -0.09581        0.955\n",
      "            67          -0.09529        0.955\n",
      "            68          -0.09479        0.955\n",
      "            69          -0.09430        0.955\n",
      "            70          -0.09382        0.955\n",
      "            71          -0.09335        0.955\n",
      "            72          -0.09289        0.955\n",
      "            73          -0.09244        0.955\n",
      "            74          -0.09200        0.955\n",
      "            75          -0.09156        0.955\n",
      "            76          -0.09114        0.955\n",
      "            77          -0.09073        0.955\n",
      "            78          -0.09032        0.955\n",
      "            79          -0.08993        0.955\n",
      "            80          -0.08954        0.955\n",
      "            81          -0.08915        0.955\n",
      "            82          -0.08878        0.955\n",
      "            83          -0.08841        0.955\n",
      "            84          -0.08805        0.955\n",
      "            85          -0.08770        0.955\n",
      "            86          -0.08735        0.955\n",
      "            87          -0.08701        0.955\n",
      "            88          -0.08667        0.955\n",
      "            89          -0.08634        0.955\n",
      "            90          -0.08602        0.955\n",
      "            91          -0.08570        0.955\n",
      "            92          -0.08539        0.955\n",
      "            93          -0.08508        0.955\n",
      "            94          -0.08478        0.955\n",
      "            95          -0.08448        0.955\n",
      "            96          -0.08419        0.955\n",
      "            97          -0.08390        0.955\n",
      "            98          -0.08362        0.955\n",
      "            99          -0.08334        0.955\n",
      "         Final          -0.08307        0.955\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.classify.MaxentClassifier.train(\n",
    "                         train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6.521 every_other3_end=='mtr' and label is 'female'\n",
      "   6.361 every_other3_beg=='Nkt' and label is 'male'\n",
      "   6.058 first3=='eti' and label is 'male'\n",
      "   6.006 every_other3_end=='Gyn' and label is 'male'\n",
      "  -5.671 first3=='ros' and label is 'male'\n",
      "   5.507 last3=='ela' and label is 'male'\n",
      "   5.451 every_other3_end=='Tmr' and label is 'female'\n",
      "   4.982 every_other3_end=='bil' and label is 'male'\n",
      "   4.915 last3=='rko' and label is 'male'\n",
      "   4.859 every_other3_end=='tin' and label is 'male'\n",
      "   4.792 last3=='rbe' and label is 'female'\n",
      "  -4.774 last2=='os' and label is 'female'\n",
      "   4.714 every_other3_end=='ir' and label is 'female'\n",
      "   4.641 every_other3_end=='hra' and label is 'male'\n",
      "   4.574 last3=='nly' and label is 'male'\n",
      "  -4.546 last2=='na' and label is 'male'\n",
      "  -4.517 every_other2_beg=='Nn' and label is 'male'\n",
      "   4.430 every_other3_beg=='Ban' and label is 'male'\n",
      "  -4.347 last3=='nne' and label is 'male'\n",
      "   4.333 every_other3_end=='ea' and label is 'male'\n",
      "   4.308 every_other3_end=='im' and label is 'male'\n",
      "   4.295 last2=='aa' and label is 'male'\n",
      "   4.295 last3=='laa' and label is 'male'\n",
      "   4.276 every_other3_end=='Fre' and label is 'female'\n",
      "   4.271 every_other3_beg=='Pte' and label is 'female'\n",
      "   4.263 every_other3_beg=='Msh' and label is 'male'\n",
      "   4.263 every_other3_end=='ica' and label is 'male'\n",
      "  -4.232 first3=='tha' and label is 'female'\n",
      "   4.220 every_other3_end=='hiy' and label is 'male'\n",
      "  -4.214 first2=='hu' and label is 'female'\n",
      "   4.209 every_other3_beg=='Sod' and label is 'female'\n",
      "   4.205 every_other3_beg=='Kih' and label is 'male'\n",
      "   4.205 every_other3_end=='iha' and label is 'male'\n",
      "  -4.203 last=='k' and label is 'female'\n",
      "   4.179 every_other3_end=='Csa' and label is 'male'\n",
      "  -4.164 every_other2_end=='la' and label is 'male'\n",
      "   4.161 every_other3_end=='rot' and label is 'female'\n",
      "   4.085 first3=='lyl' and label is 'male'\n",
      "  -4.074 last=='a' and label is 'male'\n",
      "   4.032 first3=='evy' and label is 'female'\n",
      "  -4.027 last2=='us' and label is 'female'\n",
      "   4.023 last3=='ars' and label is 'male'\n",
      "  -4.020 last2=='ra' and label is 'male'\n",
      "  -3.992 every_other2_end=='ea' and label is 'male'\n",
      "   3.991 every_other3_beg=='Tf' and label is 'female'\n",
      "   3.988 every_other3_beg=='Hsh' and label is 'female'\n",
      "   3.974 every_other3_beg=='Prh' and label is 'male'\n",
      "   3.961 every_other3_end=='ave' and label is 'male'\n",
      "  -3.948 every_other3_end=='hr' and label is 'male'\n",
      "   3.941 every_other3_beg=='Goa' and label is 'male'\n"
     ]
    }
   ],
   "source": [
    "# look at most informative features\n",
    "classifier.show_most_informative_features(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812\n"
     ]
    }
   ],
   "source": [
    "# classifer accuracy on validation set\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual, guess, name: \n",
      "\n",
      "('female', 'male', 'Winonah')\n",
      "('female', 'male', 'Eada')\n",
      "('female', 'male', 'Jennette')\n",
      "('female', 'male', 'Kylila')\n",
      "('female', 'male', 'Sonni')\n",
      "('female', 'male', 'Karita')\n",
      "('female', 'male', 'Rorie')\n",
      "('female', 'male', 'Flora')\n",
      "('female', 'male', 'Mechelle')\n",
      "('female', 'male', 'Damita')\n",
      "('female', 'male', 'Deloria')\n",
      "('female', 'male', 'Rhody')\n",
      "('female', 'male', 'Abbey')\n",
      "('female', 'male', 'Bonnie')\n",
      "('female', 'male', 'Leesa')\n",
      "('female', 'male', 'Vanessa')\n",
      "('female', 'male', 'Rubia')\n",
      "('female', 'male', 'Shaylyn')\n",
      "('female', 'male', 'Tessie')\n",
      "('female', 'male', 'Monika')\n",
      "('female', 'male', 'Susie')\n",
      "('female', 'male', 'Larina')\n",
      "('female', 'male', 'Margaretta')\n",
      "('female', 'male', 'Erminia')\n",
      "('female', 'male', 'Vickie')\n",
      "('female', 'male', 'Seana')\n",
      "('female', 'male', 'Tandie')\n",
      "('female', 'male', 'Lira')\n",
      "('female', 'male', 'Elise')\n",
      "('female', 'male', 'Alexis')\n",
      "('female', 'male', 'Hyacinthe')\n",
      "('female', 'male', 'Leoline')\n",
      "('female', 'male', 'Joann')\n",
      "('female', 'male', 'Arlee')\n",
      "('female', 'male', 'Cathi')\n",
      "('female', 'male', 'Billi')\n",
      "('female', 'male', 'Sheri')\n",
      "('female', 'male', 'Ophelia')\n",
      "('female', 'male', 'Karin')\n",
      "('female', 'male', 'Benny')\n",
      "('female', 'male', 'Orelle')\n",
      "('female', 'male', 'Randa')\n",
      "('female', 'male', 'Lucine')\n",
      "('female', 'male', 'Dolora')\n",
      "('female', 'male', 'Kissee')\n",
      "('female', 'male', 'Renata')\n",
      "('female', 'male', 'Davita')\n",
      "('female', 'male', 'Laural')\n",
      "('female', 'male', 'Elianore')\n",
      "('female', 'male', 'Stormy')\n",
      "('female', 'male', 'Marillin')\n",
      "('female', 'male', 'Blinnie')\n",
      "('female', 'male', 'Linn')\n",
      "('female', 'male', 'Sibby')\n",
      "('female', 'male', 'Elsie')\n",
      "('female', 'male', 'Selie')\n",
      "('female', 'male', 'Joanie')\n",
      "('female', 'male', 'Cody')\n",
      "('female', 'male', 'Gwenore')\n",
      "('female', 'male', 'Haley')\n",
      "('female', 'male', 'Tillie')\n",
      "('female', 'male', 'Merissa')\n",
      "('female', 'male', 'Ashlie')\n",
      "('female', 'male', 'Mitra')\n",
      "('female', 'male', 'Albina')\n",
      "('female', 'male', 'Claribel')\n",
      "('female', 'male', 'Jania')\n",
      "('female', 'male', 'Robyn')\n",
      "('female', 'male', 'Frannie')\n",
      "('female', 'male', 'Tiffany')\n",
      "('female', 'male', 'Frankie')\n",
      "('female', 'male', 'Rosy')\n",
      "('female', 'male', 'Kim')\n",
      "('female', 'male', 'Pammie')\n",
      "('female', 'male', 'Maritsa')\n",
      "('female', 'male', 'Lamb')\n",
      "('female', 'male', 'Paulina')\n",
      "('female', 'male', 'Larisa')\n",
      "('female', 'male', 'Melita')\n",
      "('female', 'male', 'Rubi')\n",
      "('female', 'male', 'Jude')\n",
      "('female', 'male', 'Ruthy')\n",
      "('female', 'male', 'Corina')\n",
      "('female', 'male', 'Elnore')\n",
      "('female', 'male', 'Ardelia')\n",
      "('female', 'male', 'Joby')\n",
      "('female', 'male', 'Kynthia')\n",
      "('female', 'male', 'Madella')\n",
      "('female', 'male', 'Marian')\n",
      "('female', 'male', 'Maddy')\n",
      "('female', 'male', 'Georgianne')\n",
      "('female', 'male', 'Laurette')\n",
      "('female', 'male', 'Joycelin')\n",
      "('female', 'male', 'Chrysler')\n",
      "('female', 'male', 'Marilyn')\n",
      "('female', 'male', 'Carmela')\n",
      "('female', 'male', 'Mollie')\n",
      "('female', 'male', 'Cyb')\n",
      "('female', 'male', 'Marin')\n",
      "('female', 'male', 'Eva')\n",
      "('female', 'male', 'Franky')\n",
      "('female', 'male', 'Dorene')\n",
      "('female', 'male', 'Honey')\n",
      "('female', 'male', 'Cherlyn')\n",
      "('female', 'male', 'Quentin')\n",
      "('female', 'male', 'Marjie')\n",
      "('female', 'male', 'Karrah')\n",
      "('female', 'male', 'Cherrita')\n",
      "('female', 'male', 'Thelma')\n",
      "('female', 'male', 'Rosie')\n",
      "('female', 'male', 'Xaviera')\n",
      "('female', 'male', 'Orelia')\n",
      "('female', 'male', 'Ann-Marie')\n",
      "('female', 'male', 'Elysee')\n",
      "('female', 'male', 'Delia')\n",
      "('female', 'male', 'Reyna')\n",
      "('female', 'male', 'Jess')\n",
      "('female', 'male', 'Raphaela')\n",
      "('female', 'male', 'Melly')\n",
      "('female', 'male', 'Astra')\n",
      "('female', 'male', 'Camile')\n",
      "('female', 'male', 'Milly')\n",
      "('female', 'male', 'Karyl')\n",
      "('female', 'male', 'Burta')\n",
      "('female', 'male', 'Stefanie')\n",
      "('female', 'male', 'Charleen')\n",
      "('female', 'male', 'Faina')\n",
      "('female', 'male', 'Clarie')\n",
      "('female', 'male', 'Elyse')\n",
      "('female', 'male', 'Zonnya')\n",
      "('female', 'male', 'Viviyan')\n",
      "('female', 'male', 'Teryl')\n",
      "('female', 'male', 'Sybila')\n",
      "('female', 'male', 'Harriette')\n",
      "('female', 'male', 'Con')\n",
      "('female', 'male', 'Melli')\n",
      "('female', 'male', 'Charline')\n",
      "('female', 'male', 'Zorine')\n",
      "('female', 'male', 'Gwen')\n",
      "('female', 'male', 'Malinda')\n",
      "('female', 'male', 'Elisa')\n",
      "('female', 'male', 'Elsy')\n",
      "('female', 'male', 'Mia')\n",
      "('female', 'male', 'Stevana')\n",
      "('female', 'male', 'Emera')\n",
      "('female', 'male', 'Albertina')\n",
      "('female', 'male', 'Dorothee')\n",
      "('female', 'male', 'Marcile')\n",
      "('female', 'male', 'Sheril')\n",
      "('female', 'male', 'Anallese')\n",
      "('female', 'male', 'Gaylene')\n",
      "('female', 'male', 'Elspeth')\n",
      "('female', 'male', 'Sadella')\n",
      "('female', 'male', 'Aile')\n",
      "('female', 'male', 'Jamie')\n",
      "('female', 'male', 'Daniela')\n",
      "('female', 'male', 'Suzan')\n",
      "('female', 'male', 'Sidonnie')\n",
      "('female', 'male', 'Valeda')\n",
      "('female', 'male', 'Loralie')\n",
      "('female', 'male', 'Dolli')\n",
      "('female', 'male', 'Kelsey')\n",
      "('female', 'male', 'Zita')\n",
      "('female', 'male', 'Atalanta')\n",
      "('female', 'male', 'Shelbi')\n",
      "('female', 'male', 'Daveta')\n",
      "('female', 'male', 'Kiley')\n",
      "('female', 'male', 'Loise')\n",
      "('female', 'male', 'Courtney')\n",
      "('female', 'male', 'Valencia')\n",
      "('female', 'male', 'Pavla')\n",
      "('female', 'male', 'Ortensia')\n",
      "('female', 'male', 'Linnie')\n",
      "('female', 'male', 'Oralla')\n",
      "('female', 'male', 'Flo')\n",
      "('female', 'male', 'Luciana')\n",
      "('female', 'male', 'Yettie')\n",
      "('female', 'male', 'Peta')\n",
      "('female', 'male', 'Dorcas')\n",
      "('female', 'male', 'Fulvia')\n",
      "('female', 'male', 'Paula-Grace')\n",
      "('female', 'male', 'Jewel')\n",
      "('female', 'male', 'Tammi')\n",
      "('female', 'male', 'Danna')\n",
      "('female', 'male', 'Fleur')\n",
      "('female', 'male', 'Fawna')\n",
      "('female', 'male', 'Denice')\n",
      "('female', 'male', 'Teddi')\n",
      "('female', 'male', 'Nicole')\n",
      "('female', 'male', 'Aveline')\n",
      "('female', 'male', 'Berthe')\n",
      "('female', 'male', 'Konstanze')\n",
      "('female', 'male', 'Brandice')\n",
      "('female', 'male', 'Ilysa')\n",
      "('female', 'male', 'Maire')\n",
      "('female', 'male', 'Kelcie')\n",
      "('female', 'male', 'Rachelle')\n",
      "('female', 'male', 'Brita')\n",
      "('female', 'male', 'Cordey')\n",
      "('female', 'male', 'Claude')\n",
      "('female', 'male', 'Meris')\n",
      "('female', 'male', 'Sybyl')\n",
      "('female', 'male', 'Guillema')\n",
      "('female', 'male', 'Lilllie')\n",
      "('female', 'male', 'Megan')\n",
      "('female', 'male', 'Rodie')\n",
      "('female', 'male', 'Mureil')\n",
      "('female', 'male', 'Conchita')\n",
      "('female', 'male', 'Olenka')\n",
      "('female', 'male', 'Aeriel')\n",
      "('female', 'male', 'Jessa')\n",
      "('female', 'male', 'Avis')\n",
      "('female', 'male', 'Kyla')\n",
      "('female', 'male', 'Consuelo')\n",
      "('female', 'male', 'Belle')\n",
      "('female', 'male', 'Lotta')\n",
      "('female', 'male', 'Danila')\n",
      "('female', 'male', 'Elly')\n",
      "('female', 'male', 'Reeba')\n",
      "('female', 'male', 'Halette')\n",
      "('female', 'male', 'Verile')\n",
      "('female', 'male', 'Roby')\n",
      "('female', 'male', 'Alane')\n",
      "('female', 'male', 'Marja')\n",
      "('female', 'male', 'Jeniffer')\n",
      "('female', 'male', 'Nevsa')\n",
      "('female', 'male', 'Katlin')\n",
      "('female', 'male', 'Lanette')\n",
      "('female', 'male', 'Lilla')\n",
      "('female', 'male', 'Julia')\n",
      "('female', 'male', 'Van')\n",
      "('female', 'male', 'Ketti')\n",
      "('female', 'male', 'Vinnie')\n",
      "('female', 'male', 'Ricki')\n",
      "('female', 'male', 'Willetta')\n",
      "('female', 'male', 'Cornelle')\n",
      "('female', 'male', 'Hester')\n",
      "('female', 'male', 'Merrili')\n",
      "('female', 'male', 'Agatha')\n",
      "('female', 'male', 'Glynda')\n",
      "('female', 'male', 'Ardenia')\n",
      "('female', 'male', 'Gusti')\n",
      "('female', 'male', 'Kimberlyn')\n",
      "('female', 'male', 'Ava')\n",
      "('female', 'male', 'Dahlia')\n",
      "('female', 'male', 'Stephine')\n",
      "('female', 'male', 'Pacifica')\n",
      "('female', 'male', 'Susanne')\n",
      "('female', 'male', 'Bertina')\n",
      "('female', 'male', 'Janie')\n",
      "('female', 'male', 'Sinead')\n",
      "('female', 'male', 'Daphene')\n",
      "('female', 'male', 'Devin')\n",
      "('female', 'male', 'Sandye')\n",
      "('female', 'male', 'Sharity')\n",
      "('female', 'male', 'Susy')\n",
      "('female', 'male', 'Alvina')\n",
      "('female', 'male', 'George')\n",
      "('female', 'male', 'Avril')\n",
      "('female', 'male', 'Guillemette')\n",
      "('female', 'male', 'Kelcy')\n",
      "('female', 'male', 'Kakalina')\n",
      "('female', 'male', 'Ralina')\n",
      "('female', 'male', 'Lana')\n",
      "('female', 'male', 'Germain')\n",
      "('female', 'male', 'Eartha')\n",
      "('female', 'male', 'Tally')\n",
      "('female', 'male', 'Randie')\n",
      "('female', 'male', 'Rianon')\n",
      "('female', 'male', 'Merrile')\n",
      "('female', 'male', 'Nelli')\n",
      "('female', 'male', 'Joete')\n",
      "('female', 'male', 'Felicia')\n",
      "('female', 'male', 'Odele')\n",
      "('female', 'male', 'Cissy')\n",
      "('female', 'male', 'Lottie')\n",
      "('female', 'male', 'Joly')\n",
      "('female', 'male', 'Queenie')\n",
      "('female', 'male', 'Kessia')\n",
      "('female', 'male', 'Lydia')\n",
      "('female', 'male', 'Dorita')\n",
      "('female', 'male', 'Christabel')\n",
      "('female', 'male', 'Bidget')\n",
      "('female', 'male', 'Florence')\n",
      "('female', 'male', 'Amberly')\n",
      "('female', 'male', 'Bonni')\n",
      "('female', 'male', 'Alla')\n",
      "('female', 'male', 'Charlot')\n",
      "('female', 'male', 'Page')\n",
      "('female', 'male', 'Simone')\n",
      "('female', 'male', 'Liz')\n",
      "('female', 'male', 'Jannelle')\n",
      "('female', 'male', 'Kamilah')\n",
      "('female', 'male', 'Milka')\n",
      "('female', 'male', 'Neile')\n",
      "('female', 'male', 'Oona')\n",
      "('female', 'male', 'Appolonia')\n",
      "('female', 'male', 'Winnie')\n"
     ]
    }
   ],
   "source": [
    "# look at names that were mis-classified\n",
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    #print(name)\n",
    "    guess = classifier2.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append( (tag, guess, name) )\n",
    "\n",
    "print('actual, guess, name: \\n')\n",
    "for x in errors:\n",
    "    print(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the classifer on the test set to see accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816\n"
     ]
    }
   ],
   "source": [
    "# classifer accuracy on validation set\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "Maximum entropy classifier: We can see that it uses an iterative method to maximize the performance of the training corpus classification. In this case the default number of iteration was 100, which is reasonable for our dataset.\n",
    "This is why it takes a long time to train a huge dataset and could also explain why it is not as popular.\n",
    "Another drawback with this classifer is that can only answer the questions about the conditional probabilities of the feature and label compared to the naive bayes.\n",
    " \n",
    "However, this classifier gives us options to associate more than one feature with a given label and vice versa. Another advantage is that the classifier does not require a vast amount of data like decision trees as long as the dataset has a high entropy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "http://www.nltk.org/howto/corpus.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
